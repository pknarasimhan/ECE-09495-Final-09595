
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-redhat-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> rm(list=ls())
> 
> #Age, BMI, BSA, Height, Weight, Preferred Term, AE related (Y/N) and SAE related (Y/N)
> 
> #read data
> d1<-read.csv("pretreated.csv", header=TRUE, sep=",", stringsAsFactors = FALSE) 
> dim(d1) #1633 x 1315
[1] 1633 1315
> 
> excl_col<-c("COLITIS","HYPOTHYROIDISM","PNEUMONIA","HEPATITIS")
> d1<-d1[,setdiff(colnames(d1),excl_col)]
> dim(d1) #1633 x 1311
[1] 1633 1311
> 
> d2<-na.omit(d1[,c(1:9,1310)])
> dim(d2) #1161 x10 
[1] 1161   10
> d3<-merge(d2, d1[,10:1311], by="Subject_ID")
> dim(d3)  #1161 x 1311
[1] 1161 1311
> table(d3$immflag) #1014/147

  no  yes 
1014  147 
> 
> d3[,c(3:6,1311)]<-lapply(d3[,c(3:6,1311)],factor)
> d3[,c(11:1310)]<-lapply(d3[,c(11:1310)],function(a){
+   ifelse(is.na(a),0,ifelse(a=="GRADE 1",1,ifelse(a=="GRADE 2",2,ifelse(a=="GRADE 3",3,ifelse(a=="GRADE 4",4,5)))))
+ })
> 
> d3<-d3[,apply(d3[,c(11:1310)],2,sum)!=0]  #keep only the AE is recorded
> dim(d3) #1161 x 1169
[1] 1161 1169
> d3<-d3[apply(d3[,c(11:1168)],1,sum)!=0,]
> dim(d3) #1153 x 1169
[1] 1153 1169
> #write.csv(d3, file="pretreated2.csv",row.names = FALSE)
> d4<-d3[,-1] #remove ID
> colnames(d4)[10:1167]<-paste("A",seq(10:1167),sep="")
> 
> ###################################################################
> #     CARET
> ###################################################################
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> 
> #This model here includes Recursive Feature Elimination first
> set.seed(222)
> # define the control using a random forest selection function
> control <- rfeControl(functions=rfFuncs, method="cv", number=10)
> system.time(results <- rfe(d4[,-1168], d4[,1168], sizes=c(5, 10, 50, 100, 200), rfeControl=control))
Loading required package: randomForest
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:ggplot2':

    margin

    user   system  elapsed 
1187.720    0.284 1189.886 
> 
> nzv <- nearZeroVar(d4[,-1168])
> filteredDescr <- d4[, -nzv]
> dim(filteredDescr)
[1] 1153   29
> 
> #Append the important ones from RFE
> confirmed<-predictors(results)
> filteredDescr<-d4[,c(confirmed,setdiff(colnames(filteredDescr),confirmed))]
> dim(filteredDescr)
[1] 1153 1168
> 
> #Append the important ones from Boruta
> #confirmed<-c("A70", "A182", "A218", "A292", "A412", "A428", "A494", "A703", "A915", "A923", "A1047", "A1144")
> #filteredDescr<-cbind(filteredDescr,d4[,confirmed])
> 
> set.seed(222)
> testidx <- createDataPartition(filteredDescr$immflag, p = .3)[[1]]
> training <- filteredDescr[-testidx,]
> testing <-filteredDescr[testidx,] 
> 
> ## Determine the predictor names
> predictors <- names(training)[names(training) != c("immflag")]
> fiveStats <- function(...) c(twoClassSummary(...), defaultSummary(...))
> ctrl <- trainControl(method = "cv", 
+                      number = 10, 
+                      classProbs = TRUE,
+                      returnResamp = "all",
+                      summaryFunction = fiveStats,
+                      verboseIter = TRUE)
> 
> #grid <- expand.grid(.alpha=c(0,1),.lambda=seq(0,0.05,by=0.01)) #glmnet
> #grid <- expand.grid(.size=c(1,5,10,15,20),.decay=c(0.01,0.001,0.1)) #nnet
> #grid <- expand.grid(.C=10^seq(4,-2,length=5),.sigma=10^seq(1,-4,length=5))#SVM
> #grid <- expand.grid(.interaction.depth = seq(1, 10, by = 2), 
> #                    .n.trees = seq(20, 120, by = 20), 
> #                    .shrinkage = c(0.01, 0.01, 0.1), .n.minobsinnode=10) #gbm
> #grid <- expand.grid(k = c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16)) 
> 
> set.seed(222)
> up_train <- upSample(x = training[,predictors],
+                      y = training$immflag,
+                      ## keep the class variable name the same:
+                      yname = "immflag")
> table(up_train$immflag)

 no yes 
706 706 
> 
> set.seed(222)
> down_train<- downSample(x = training[,predictors],
+                         y = training$immflag,
+                         ## keep the class variable name the same:
+                         yname = "immflag")
> table(down_train$immflag)

 no yes 
100 100 
> 
> library(DMwR)
Loading required package: grid

Attaching package: 'DMwR'

The following object is masked from 'package:plyr':

    join

> set.seed(222)
> smote_train <- SMOTE(immflag ~ ., data  = training)
> table(smote_train$immflag)

 no yes 
400 300 
> 
> ###############################################################################
> set.seed(222)
> orig_fit <- train(data.matrix(training[,predictors]), training$immflag,
+                   #method = 'glmnet',
+                   #method = 'nnet',
+                   method = 'rpart',
+                   #method = "svmRadial",
+                   #method = 'nb',
+                   #method ='gbm',
+                   #method = 'rf',
+                   #method ="knn",
+                   metric = "ROC",
+                   #tuneGrid = grid, 
+                   trControl = ctrl)
Loading required package: rpart
+ Fold01: cp=0.006667 
- Fold01: cp=0.006667 
+ Fold02: cp=0.006667 
- Fold02: cp=0.006667 
+ Fold03: cp=0.006667 
- Fold03: cp=0.006667 
+ Fold04: cp=0.006667 
- Fold04: cp=0.006667 
+ Fold05: cp=0.006667 
- Fold05: cp=0.006667 
+ Fold06: cp=0.006667 
- Fold06: cp=0.006667 
+ Fold07: cp=0.006667 
- Fold07: cp=0.006667 
+ Fold08: cp=0.006667 
- Fold08: cp=0.006667 
+ Fold09: cp=0.006667 
- Fold09: cp=0.006667 
+ Fold10: cp=0.006667 
- Fold10: cp=0.006667 
Aggregating results
Selecting tuning parameters
Fitting cp = 0.008 on full training set
> varImp(orig_fit)
rpart variable importance

  only 20 most important variables shown (out of 1167)

      Overall
A494   100.00
A268    64.46
A496    45.45
A1129   42.62
A675    41.14
A19      0.00
A848     0.00
A207     0.00
A818     0.00
A812     0.00
A816     0.00
A1151    0.00
A584     0.00
A645     0.00
A540     0.00
A1104    0.00
A600     0.00
A897     0.00
A561     0.00
A150     0.00
> #summary(orig_fit) #gbm
> plot(orig_fit, metric="ROC")
> confusionMatrix(predict(orig_fit, newdata=data.matrix(testing[, predictors])),testing$immflag)
Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  302  44
       yes   1   0
                                          
               Accuracy : 0.8703          
                 95% CI : (0.8303, 0.9038)
    No Information Rate : 0.8732          
    P-Value [Acc > NIR] : 0.6029          
                                          
                  Kappa : -0.0057         
 Mcnemar's Test P-Value : 3.825e-10       
                                          
            Sensitivity : 0.9967          
            Specificity : 0.0000          
         Pos Pred Value : 0.8728          
         Neg Pred Value : 0.0000          
             Prevalence : 0.8732          
         Detection Rate : 0.8703          
   Detection Prevalence : 0.9971          
      Balanced Accuracy : 0.4983          
                                          
       'Positive' Class : no              
                                          
> 
> set.seed(222)
> down_outside <- train(data.matrix(down_train[,predictors]), down_train$immflag,
+                       #method='glmnet',
+                       #method='nnet',
+                       method = 'rpart',
+                       #method='nb',
+                       #method='rf',
+                       #method = "svmRadial",
+                       #method = 'knn',
+                       #method='gbm',
+                       metric = "ROC",
+                       #tuneGrid = grid,
+                       trControl = ctrl)
+ Fold01: cp=0.05 
- Fold01: cp=0.05 
+ Fold02: cp=0.05 
- Fold02: cp=0.05 
+ Fold03: cp=0.05 
- Fold03: cp=0.05 
+ Fold04: cp=0.05 
- Fold04: cp=0.05 
+ Fold05: cp=0.05 
- Fold05: cp=0.05 
+ Fold06: cp=0.05 
- Fold06: cp=0.05 
+ Fold07: cp=0.05 
- Fold07: cp=0.05 
+ Fold08: cp=0.05 
- Fold08: cp=0.05 
+ Fold09: cp=0.05 
- Fold09: cp=0.05 
+ Fold10: cp=0.05 
- Fold10: cp=0.05 
Aggregating results
Selecting tuning parameters
Fitting cp = 0.05 on full training set
> 
> varImp(down_outside)
rpart variable importance

  only 20 most important variables shown (out of 1167)

                Overall
baseline_weight  100.00
baseline_bsa_c    80.05
A127              79.09
A268              68.26
baseline_bmi      43.62
A673              39.62
A234              31.72
A65                0.00
A349               0.00
A175               0.00
A146               0.00
A1114              0.00
A416               0.00
A1065              0.00
A400               0.00
A557               0.00
A1026              0.00
A733               0.00
A776               0.00
A29                0.00
> plot(down_outside, metric="ROC")
> confusionMatrix(predict(down_outside, newdata=data.matrix(testing[, predictors])),testing$immflag)
Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  226  31
       yes  77  13
                                          
               Accuracy : 0.6888          
                 95% CI : (0.6371, 0.7371)
    No Information Rate : 0.8732          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.0286          
 Mcnemar's Test P-Value : 1.49e-05        
                                          
            Sensitivity : 0.7459          
            Specificity : 0.2955          
         Pos Pred Value : 0.8794          
         Neg Pred Value : 0.1444          
             Prevalence : 0.8732          
         Detection Rate : 0.6513          
   Detection Prevalence : 0.7406          
      Balanced Accuracy : 0.5207          
                                          
       'Positive' Class : no              
                                          
> 
> set.seed(222)
> up_outside <- train(data.matrix(up_train[,predictors]), up_train$immflag,
+                     #method = 'glmnet',
+                     #method = 'nnet',
+                     method = 'rpart',
+                     #method = 'svmRadial',
+                     #method = 'nb',
+                     #method = 'knn',
+                     #method = 'rf',
+                     #method = 'gbm',
+                     metric = "ROC",
+                     #tuneGrid = grid, #glmnet and nnet use only
+                     trControl = ctrl)
+ Fold01: cp=0.02833 
- Fold01: cp=0.02833 
+ Fold02: cp=0.02833 
- Fold02: cp=0.02833 
+ Fold03: cp=0.02833 
- Fold03: cp=0.02833 
+ Fold04: cp=0.02833 
- Fold04: cp=0.02833 
+ Fold05: cp=0.02833 
- Fold05: cp=0.02833 
+ Fold06: cp=0.02833 
- Fold06: cp=0.02833 
+ Fold07: cp=0.02833 
- Fold07: cp=0.02833 
+ Fold08: cp=0.02833 
- Fold08: cp=0.02833 
+ Fold09: cp=0.02833 
- Fold09: cp=0.02833 
+ Fold10: cp=0.02833 
- Fold10: cp=0.02833 
Aggregating results
Selecting tuning parameters
Fitting cp = 0.0283 on full training set
> 
> varImp(up_outside)
rpart variable importance

  only 20 most important variables shown (out of 1167)

                Overall
A268             100.00
A618              86.12
A524              81.39
A428              78.38
A234              75.31
A1092             71.86
A127              70.60
A675              64.36
A494              53.78
Age_DRV           52.05
baseline_height   40.95
A824              33.28
A605               0.00
A1013              0.00
A215               0.00
A770               0.00
A1133              0.00
A1116              0.00
A304               0.00
A277               0.00
> plot(up_outside, metric="ROC")
> confusionMatrix(predict(up_outside, newdata=data.matrix(testing[, predictors])),testing$immflag)
Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  156  17
       yes 147  27
                                          
               Accuracy : 0.5274          
                 95% CI : (0.4734, 0.5809)
    No Information Rate : 0.8732          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.0568          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.5149          
            Specificity : 0.6136          
         Pos Pred Value : 0.9017          
         Neg Pred Value : 0.1552          
             Prevalence : 0.8732          
         Detection Rate : 0.4496          
   Detection Prevalence : 0.4986          
      Balanced Accuracy : 0.5642          
                                          
       'Positive' Class : no              
                                          
> 
> set.seed(222)
> smote_outside <- train(data.matrix(smote_train[,predictors]), smote_train$immflag,
+                        #method='glmnet',
+                        #method='nnet',
+                        method = 'rpart',
+                        #method ='svmRadial',
+                        #method='nb',
+                        #method='knn',
+                        #method='gbm',
+                        #method='rf',
+                        metric = "ROC",
+                        #tuneGrid = grid,
+                        trControl = ctrl)
+ Fold01: cp=0.09 
- Fold01: cp=0.09 
+ Fold02: cp=0.09 
- Fold02: cp=0.09 
+ Fold03: cp=0.09 
- Fold03: cp=0.09 
+ Fold04: cp=0.09 
- Fold04: cp=0.09 
+ Fold05: cp=0.09 
- Fold05: cp=0.09 
+ Fold06: cp=0.09 
- Fold06: cp=0.09 
+ Fold07: cp=0.09 
- Fold07: cp=0.09 
+ Fold08: cp=0.09 
- Fold08: cp=0.09 
+ Fold09: cp=0.09 
- Fold09: cp=0.09 
+ Fold10: cp=0.09 
- Fold10: cp=0.09 
Aggregating results
Selecting tuning parameters
Fitting cp = 0.09 on full training set
> 
> varImp(smote_outside)
rpart variable importance

  only 20 most important variables shown (out of 1167)

      Overall
A460   100.00
A586    85.83
A463    71.42
A127    59.35
A244    55.83
A993    49.65
A735    48.14
A859    46.08
A816    45.96
A749     0.00
A225     0.00
A1024    0.00
A1044    0.00
A742     0.00
A615     0.00
A668     0.00
A990     0.00
A87      0.00
A504     0.00
A500     0.00
> plot(smote_outside, metric="ROC")
> confusionMatrix(predict(smote_outside, newdata=data.matrix(testing[, predictors])),testing$immflag)
Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  274  37
       yes  29   7
                                          
               Accuracy : 0.8098          
                 95% CI : (0.7645, 0.8497)
    No Information Rate : 0.8732          
    P-Value [Acc > NIR] : 0.9997          
                                          
                  Kappa : 0.0687          
 Mcnemar's Test P-Value : 0.3889          
                                          
            Sensitivity : 0.9043          
            Specificity : 0.1591          
         Pos Pred Value : 0.8810          
         Neg Pred Value : 0.1944          
             Prevalence : 0.8732          
         Detection Rate : 0.7896          
   Detection Prevalence : 0.8963          
      Balanced Accuracy : 0.5317          
                                          
       'Positive' Class : no              
                                          
> 
> outside_models <- list(original = orig_fit,
+                        down = down_outside,
+                        up = up_outside,
+                        SMOTE = smote_outside
+ )
> 
> outside_resampling <- resamples(outside_models)
Warning messages:
1: In resamples.default(outside_models) :
  'original' did not have 'returnResamp="final"; the optimal tuning parameters are used
2: In resamples.default(outside_models) :
  'down' did not have 'returnResamp="final"; the optimal tuning parameters are used
3: In resamples.default(outside_models) :
  'up' did not have 'returnResamp="final"; the optimal tuning parameters are used
4: In resamples.default(outside_models) :
  'SMOTE' did not have 'returnResamp="final"; the optimal tuning parameters are used
> summary(outside_resampling, metric = "ROC")

Call:
summary.resamples(object = outside_resampling, metric = "ROC")

Models: original, down, up, SMOTE 
Number of resamples: 10 

ROC 
           Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
original 0.3965  0.4744 0.5366 0.5346  0.5805 0.7028    0
down     0.3900  0.5275 0.5675 0.5740  0.6312 0.7500    0
up       0.6508  0.6730 0.6967 0.6963  0.7216 0.7394    0
SMOTE    0.7083  0.7548 0.7758 0.7816  0.8119 0.8842    0

> 
> #Real test check compared to resampling results
> test_roc <- function(model, data) {
+   library(pROC)
+   roc_obj <- roc(data$immflag,
+                  predict(model, data.matrix(data[,predictors]), type = "prob")[,1])
+   ci(roc_obj)
+ }
> 
> outside_test <- lapply(outside_models, test_roc, data = testing)
Type 'citation("pROC")' for a citation.

Attaching package: 'pROC'

The following objects are masked from 'package:stats':

    cov, smooth, var

> outside_test <- lapply(outside_test, as.vector)
> outside_test <- do.call("rbind", outside_test)
> colnames(outside_test) <- c("lower", "ROC", "upper")
> outside_test <- as.data.frame(outside_test)
> outside_test
             lower       ROC     upper
original 0.4984159 0.5016502 0.5048844
down     0.4137889 0.4847735 0.5557581
up       0.4884374 0.5745575 0.6606775
SMOTE    0.4113328 0.4684593 0.5255859
> 
> 
> # Plot ROC curve for prediction of the independent validation set
> roc.curve <- plot.roc(testing$immflag, predict(orig_fit, data.matrix(testing[,predictors]), type = "prob")[,1], percent=TRUE, ci=TRUE)
> roc2<-roc(testing$immflag, predict(orig_fit, data.matrix(testing[,predictors]), type = "prob")[,1])
> plot(roc2, legacy.axes = TRUE)

Call:
roc.default(response = testing$immflag, predictor = predict(orig_fit,     data.matrix(testing[, predictors]), type = "prob")[, 1])

Data: predict(orig_fit, data.matrix(testing[, predictors]), type = "prob")[, 1] in 303 controls (testing$immflag no) < 44 cases (testing$immflag yes).
Area under the curve: 0.5017
> plot(roc2, print.thres="best", print.thres.best.method="closest.topleft")

Call:
roc.default(response = testing$immflag, predictor = predict(orig_fit,     data.matrix(testing[, predictors]), type = "prob")[, 1])

Data: predict(orig_fit, data.matrix(testing[, predictors]), type = "prob")[, 1] in 303 controls (testing$immflag no) < 44 cases (testing$immflag yes).
Area under the curve: 0.5017
> result.coords<-coords(roc2, "best", best.method = "closest.topleft",ret=c("threshold", "accuracy"))
> coords(roc2, 0.5, ret="accuracy") #Find accuracy if using 50% probability as cut-off
[1] 0.129683
> 
> plot(roc2, print.thres = c(.5), type = "S",
+      print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
+      print.thres.cex = .8, 
+      legacy.axes = TRUE)

Call:
roc.default(response = testing$immflag, predictor = predict(orig_fit,     data.matrix(testing[, predictors]), type = "prob")[, 1])

Data: predict(orig_fit, data.matrix(testing[, predictors]), type = "prob")[, 1] in 303 controls (testing$immflag no) < 44 cases (testing$immflag yes).
Area under the curve: 0.5017
> 
> # Make prediction using the best top-left cutoff.
> result.predicted.label <- factor(
+   ifelse(predict(orig_fit, data.matrix(testing[,predictors]), type = "prob")[,1] > result.coords[1], 
+          "Yes", "No"))
> table(result.predicted.label,testing$immflag)
                      
result.predicted.label  no yes
                   No    1   0
                   Yes 302  44
> 
> 
